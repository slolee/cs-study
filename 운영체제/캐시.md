## 캐시 메모리에 대해서 설명해주세요.

캐시 메모리는 CPU 의 가까운 곳에 위치하며 반복해서 참조되는 데이터를 보관해둠으로써 CPU 의 처리속도를 빠르게 하기 위해 사용되는 메모리를 말합니다.

캐시 메모리는 일반 메모리보다 비싸기 때문에 소량 존재하며, 메모리중에서 가장 빠르고 CPU 와 가장 가까이에 위치하게 됩니다.

<br>

## 캐시 메모리의 동작방식에 대해서 설명해주실 수 있나요?

CPU 가 메모리를 참조할 때 바로 메모리로 접근하는 것이 아니라 우선 캐시 메모리에 해당 메모리에 대한 정보가 있는지 확인합니다. 만약 있다면 해당 데이터를 바로 사용하고 없다면 그 때 메모리를 참조하게 됩니다. 이때 참조한 정보를 캐시 메모리에 보관함으로써 다음에 동일한 메모리 참조시에 캐시 메모리를 사용하도록 합니다.

참조하려는 메모리가 캐시 메모리에 있는 상황을 Cache Hit 이라고 하고, 캐시 메모리에 없어서 메모리를 직접 참조해야하는 상황을 Cache Miss 라고 합니다.

<br>

## 캐시 메모리가 매우 작다고 했는데, Cache Miss 가 많이 발생하는 것 아닌가요?

Cache Miss 발생 비율이 증가하게되면 결국 캐시 메모리를 한 번 더 거쳐서 메모리를 참조하는 것이기 때문에 CPU 성능 저하에 영향을 미치게 됩니다.

하지만 프로세스가 동작하면서 참조의 지역성을 가지기 때문에 작은 양의 캐시 메모리로도 충분한 성능 향상을 가져올 수 있습니다.

<br>

## 참조의 지역성이란 무엇인가요?

참조의 지역성이란 프로그램이 동작하면서 참조되는 메모리의 영역은 한 곳에 집중되어 있고, 최근에 참조된 메모리 주소는 짧은 시간 내에 다시 참조된다는 것을 의미합니다.

이렇나 참조의 지역성이 나타나는 이유는 프로그램의 많은 부분이 반복문으로 구현되어있기 때문입니다.

참조의 지역성은 크게 시간 지역성과 공간 지역성이 있고, 시간 지역성은 참조된 메모리가 짧은 시간 내에 다시 참조된다는 특징이고, 공간 지역성은 참조된 메모리 근처의 메모리가 참조되는 특징입니다.

<br>

## 그러면 캐시 메모리에 데이터를 저장하려고 하는데 꽉차있으면 어떻게 되나요?

참조된 메모리에 대한 정보를 캐시 메모리에 저장하려고 하는데 캐시 메모리가 가득 차있다면 캐시 교체 알고리즘을 사용해 특정 캐시를 삭제하고 그 자리에 해당 정보를 저장합니다.

캐시 교체 알고리즘으로는 Random, FIFO, LRU 등이 있습니다.

LRU 는 가장 오래전에 참조된 캐시 메모리를 제거하는 알고리즘으로 캐시가 참조된 순서를 관리해야하기 때문에 오버헤드가 존재한다는 단점이 있습니다. 

<br>

## 캐시가 참조된 순서는 어떻게 관리할 수 있을까요?

제 생각에는 더블드 링크드리스트와 해시를 사용해서 관리할 수 있을 것 같습니다.

더블드 링크드리스트의 Head 부터 가장 최근에 참조된 캐시 정보를 보관하고 Tail 로 갈수록 가장 오랫동안 참조되지 않은 캐시를 위치시킵니다. 이렇게 구현했을 때 새로운 메모리 정보를 캐시 메모리에 저장할 때 O(1) 의 속도로 추가할 수 있으며, LRU 알고리즘에 의해서 가장 오랫동안 참조되지 않은 캐시를 삭제하는데 걸리는 속도도 O(1) 입니다.

다만 캐시 메모리를 탐색하는데 걸리는 시간이 O(n) 이기 때문에 메모리 주소를 Key 로 사용하는 해시 테이블을 사용해 탐색 속도를 O(1) 로 줄일 수 있을 것 같습니다.

<br>

## 캐시 적중률을 계산하는 법은 무엇인가요?

캐시 적중률은 Cache Hit 횟수를 전체 접근수로 나누면 될 것 같습니다. Cache Miss 가 발생하면 CPU 가 캐시 메모리, 그리고 메모리까지 총 2번의 참조를 해야하기 때문에 오히려 성능이 안좋아 집니다.

따라서 캐시 적중률을 높이는게 CPU 의 처리속도를 높이는데 중요한 기준이 된다고 생각합니다.